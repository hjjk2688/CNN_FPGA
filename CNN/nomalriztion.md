# 양자화
#### 양자화 = 소수점 숫자를 “정수 칸”에 맞춰서 담는 것

```
0.13, -0.82, 1.47, 0.003 ...
양자화 ->
13, -82, 147, 0
```

#### 스케일팩터
스케일 팩터: 정수로 바꾸면 값이 깨지는걸 방지하는 값

스케일 팩터를 줄자비유 🔧
  - 원래 값: 실제 길이 (cm, m)
  - 정수 값: 눈금 번호
  - 스케일: 눈금 하나가 몇 cm인지

스케일 = 0.01
정수 1칸 = 실제 값 0.01

```
정수값 = 실제값 / 스케일
```

#### 예제
```
1. 학습된 값:
  - 입력 x = 0.5
  - 가중치 w = -0.8
2. 출력 값:
  - y = -0.4

3. 양자화:
  - scale_x = 0.01
  - scale_w = 0.01

  - x_int = 50
  - w_int = -80

4. FPGA에서 계산
  - y_int = 50 × (-80) = -4000

5. 의미 해석
  - 실제 y ≈ -4000 × (0.01 × 0.01) = -0.4
```
---
## 스케일 팩터

스케일 팩터는 “그 레이어에서 나올 수 있는 최대값” 기준으로 정한다

1️⃣ 왜 스케일을 정해야 하냐

정수는 범위가 정해져 있음.

예:

int8 → -128 ~ 127

int16 → -32768 ~ 32767

👉 값이 이걸 넘으면 망함 (overflow)

---

## 🧠 신경망 양자화(Quantization)

---

## 1. 양자화의 핵심 개념
양자화는 **"값의 정밀도를 줄여서 모델을 가볍고 빠르게 만드는 기술"**입니다.
* **FP32 (실수)**: 정밀하지만 계산이 무겁고 느림.
* **INT8 (정수)**: 정밀도는 낮지만 계산이 매우 가볍고 빠름.
* **목표**: 32비트 실수를 8비트 정수로 변환하여 메모리 사용량을 4배 줄이고 연산 속도를 높임.

---

## 2. 주요 공식 및 용어 (한눈에 보기)

### 🔹 스케일 팩터 (Scale Factor, $s$)
실수 공간을 정수 공간으로 매핑하기 위한 '배율'입니다.
$$s = \frac{127}{\alpha}$$
* **$\alpha$ (알파)**: 양자화 전 실수 데이터 중 가장 큰 절댓값 (기준점).
* **127**: 8비트 정수(INT8)가 표현 가능한 최대 범위 (대칭 기준).

### 🔹 양자화와 역양자화
* **양자화 ($x \rightarrow x_q$)**: 실수에 $s$를 곱하고 반올림하여 정수로 변환.
  $$x_q = \text{clip}(\text{round}(x \cdot s), -127, 127)$$
* **역양자화 ($x_q \rightarrow \hat{x}$)**: 정수를 다시 원래 크기의 실수로 복원.
  $$\hat{x} = \frac{1}{s} \cdot x_q$$

---

## 3. 왜 양자화를 하면 빠른가? (연산 원리)

가장 연산이 많은 **행렬 곱셈** 과정이 다음과 같이 변합니다.



1. **준비**: 가중치($w$)와 입력($x$)을 미리 정수로 바꿉니다 (스케일 $s_w, s_x$ 사용).
2. **연산 (핵심)**: 수백만 번의 곱셈을 **정수끼리** 수행합니다 ($\sum x_q \cdot w_q$).
3. **마무리**: 정수 연산이 다 끝난 최종 결과값에만 스케일 팩터를 **딱 한 번** 곱해줍니다.

> **결론**: 수천만 번의 실수 연산을 수천만 번의 **정수 연산** + 단 1번의 실수 연산으로 바꿉니다.

---

## 4. 핵심 요약 (이것만 기억하세요!)

1. **알파($\alpha$)**: 양자화 전 데이터 중 가장 큰 절댓값.
2. **스케일($s$)**: 실수를 정수로 압축하는 배율.
3. **효율성**: 연산 과정의 99%를 차지하는 '곱셈'을 정수로 처리해서 압도적으로 빠름.

---
*문서 업데이트: 2025-12-17*
