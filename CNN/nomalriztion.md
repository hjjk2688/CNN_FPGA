# 🧠 신경망 양자화(Quantization) & 역양자화 

이 문서는 부동소수점(FP32) 연산을 정수(INT8) 연산으로 변환하여 모델을 최적화하는 **양자화**와 그 결과를 복원하는 **역양자화**의 원리를 정리한 문서입니다.

---

## 1. 양자화란? (비유로 이해하기)
양자화는 **"소수점 숫자를 정수 칸에 맞춰서 담는 것"**입니다.

* **직관적 예시**: `0.13, -0.82, 1.47` → `13, -82, 147`
* **줄자 비유 🔧**:
  * **원래 값**: 실제 길이 (cm, m)
  * **정수 값**: 줄자의 눈금 번호
  * **스케일**: 눈금 하나가 실제 몇 cm인지 (예: 스케일 0.01 = 눈금 1칸당 0.01cm)

---

## 2. 주요 공식 및 용어

### 🔹 스케일 팩터 (Scale Factor, $s$)
실수 공간을 정수 공간(-127~127)으로 매핑하기 위한 '배율'입니다.
$$s = \frac{127}{\alpha}$$
* **$\alpha$ (알파)**: 양자화 전 실수 데이터 중 **절댓값이 가장 큰 값** (기준점).
* **설정 이유**: 정수는 범위가 제한적(INT8: -128~127)이므로, 데이터가 이 범위를 넘어 **오버플로우(Overflow)**가 발생해 값이 깨지는 것을 방지합니다.

### 🔹 양자화와 역양자화
* **양자화 ($x \rightarrow x_q$)**: 실수에 $s$를 곱하고 반올림하여 정수로 변환.
  $$x_q = \text{clip}(\text{round}(x \cdot s), -127, 127)$$
* **역양자화 ($\hat{x}$)**: 정수 연산 결과를 다시 원래 크기의 실수로 복원.
  $$\hat{x} = \frac{1}{s} \cdot x_q$$

---

## 3. ❓ 역양자화(Dequantization)가 반드시 필요한 이유

정수 연산은 빠르지만, 연산이 거듭될수록 숫자가 정수 범위를 초과하여 모델이 망가지는 것을 막기 위해 반드시 필요합니다.

1. **정수 곱셈의 폭주 방지**: 
   * 예: $127 \times 127 = 16,129$ 처럼 결과가 INT8 범위(-127~127)를 훌쩍 뛰어넘습니다.
   * 이를 다음 레이어로 넘기려면 다시 원래의 실수 스케일로 줄여주어야 합니다.
2. **연산 효율 극대화 (Re-scaling)**: 
   * 수만 번의 행렬 곱셈은 **정수**로 빠르게 처리하고, 마지막 합계 결과에만 스케일 팩터를 적용해 **실수**로 복원함으로써 속도와 정확도를 모두 잡습니다.

---

## 4. 연산 과정 예시 (FPGA/NPU 방식)

1. **준비 (학습된 값)**: 
   * 입력 $x = 0.5$, 가중치 $w = -0.8$ (실제 출력 $y = -0.4$)
2. **양자화**:
   * 스케일을 $0.01$로 가정 시: $x_{int} = 50$, $w_{int} = -80$
3. **정수 연산**:
   * $y_{int} = 50 \times (-80) = -4000$ (정수끼리 매우 빠르게 계산)
4. **역양자화 (의미 해석)**:
   * 실제 $y \approx -4000 \times (0.01 \times 0.01) = -0.4$ (마지막에만 실수 보정 수행)

---

## 5. 최종 요약 
1. **알파($\alpha$)**: 데이터 중 절댓값이 가장 큰 대장.
2. **양자화**: 수많은 실수 연산을 가벼운 **정수 연산**으로 바꾸는 것.
3. **역양자화**: 정수 연산 후 **너무 커진 결과값을 원래 실수 크기로** 되돌리는 필수 작업.

---
*최종 업데이트: 2025-12-17*
